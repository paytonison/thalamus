# Asari Brainstem v0.1.0 — Dynamic Mid-Conversation Expert Switching

*Mixture–of–Experts (MoE)* brings sparse compute to large language models by **routing tokens to expert subnetworks**. However, **most production assistants** still operate with a *single–expert–per–turn mindset* at the conversation level, **failing when users pivot mid–utterance** (e.g., “Explain SSA eligibility, then write a Python validator”). We introduce **Asari Brainstem**, a *conversation–level control stack* that (i) **detects micro–intent shifts within an utterance**, (ii) **switches or blends domain experts during generation**, and (iii) **preserves persona, safety, and task state across expert boundaries**. Two mechanisms underpin the system: **Multi–Granular Gating (MGG)**, which coordinates token–level MoE with phrase–level switching via hysteresis and budget–aware smoothing; and **On–the–Fly Expert Switching (OFES)**, an *online routing policy* driven by *semantic drift and task events.* We formalize the problem, specify algorithms, propose an evaluation suite for mid–conversation expert switching, and discuss safety, governance, and limitations.
